{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERBANDINGAN METODE KLASTER K-MEDOIDS DAN K-MEANS TERHADAP HASIL PERAMALAN KEMISKINAN DI INDONESIA MENGGUNAKAN BPNN\n",
    "### by Riansyah Fazar Ramadhan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('E:\\GitHub\\sherlock-final-project\\pooled_data.xlsx', sheet_name=\"data\")\n",
    "data = data.iloc[:, 1:4]\n",
    "data_ar = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACF-PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col= len(data.columns)\n",
    "\n",
    "# Create a figure and plot each column using a loop\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, column in enumerate(data.columns):\n",
    "    plt.subplot(num_col, 1, i + 1)\n",
    "    plt.plot(data[column])\n",
    "    plt.title(column)\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    \n",
    "    # Plot ACF\n",
    "    plot_acf(data[column], ax=axes[0], lags=67)\n",
    "    axes[0].set_title(f'ACF of {column}')\n",
    "    \n",
    "    # Plot PACF\n",
    "    plot_pacf(data[column], ax=axes[1], lags=67)\n",
    "    axes[1].set_title(f'PACF of {column}')\n",
    "    \n",
    "    # Display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Terasvirta Linearity Test\n",
    "#### Done in R language, check at terasvirtatest.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_norm = scaler.fit_transform(data)\n",
    "    return data_norm\n",
    "\n",
    "data_norm = standardize_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, batch_size, n_past, n_future, shift):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
    "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "N_PAST = 68\n",
    "N_FUTURE = 34\n",
    "SHIFT = 1\n",
    "SPLIT_TIME = 306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train = data_norm[:SPLIT_TIME,0]\n",
    "x1_test = data_norm[SPLIT_TIME:,0]\n",
    "x2_train = data_norm[:SPLIT_TIME,1]\n",
    "x2_test = data_norm[SPLIT_TIME:,1]\n",
    "x3_train = data_norm[:SPLIT_TIME,2]\n",
    "x3_test = data_norm[SPLIT_TIME:,2]\n",
    "\n",
    "train_set1 = windowed_dataset(series=x1_train, batch_size=BATCH_SIZE,\n",
    "                            n_past=N_PAST, n_future=N_FUTURE,\n",
    "                            shift=SHIFT)\n",
    "test_set1 = windowed_dataset(series=x1_test, batch_size=BATCH_SIZE,\n",
    "                            n_past=N_PAST, n_future=N_FUTURE,\n",
    "                            shift=SHIFT)\n",
    "\n",
    "train_set2 = windowed_dataset(series=x1_train, batch_size=BATCH_SIZE,\n",
    "                            n_past=N_PAST, n_future=N_FUTURE,\n",
    "                            shift=SHIFT)\n",
    "test_set2 = windowed_dataset(series=x1_test, batch_size=BATCH_SIZE,\n",
    "                            n_past=N_PAST, n_future=N_FUTURE,\n",
    "                            shift=SHIFT)\n",
    "\n",
    "train_set3 = windowed_dataset(series=x1_train, batch_size=BATCH_SIZE,\n",
    "                            n_past=N_PAST, n_future=N_FUTURE,\n",
    "                            shift=SHIFT)\n",
    "test_set3 = windowed_dataset(series=x1_test, batch_size=BATCH_SIZE,\n",
    "                            n_past=N_PAST, n_future=N_FUTURE,\n",
    "                            shift=SHIFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_true = tf.maximum(y_true, epsilon)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = tf.reduce_mean(tf.abs((y_true - y_pred) / y_true))\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(train_dataset, test_dataset, epoch, loss_function, metrics, optimizer, activation, hidden_node):\n",
    "        model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Input(shape=(5,)),\n",
    "                tf.keras.layers.Dense(hidden_node, activation=activation),\n",
    "                tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "        model.compile(loss=loss_function, optimizer=optimizer, metrics=[metrics])\n",
    "        history = model.fit(train_dataset, epochs=epoch, validation_data=test_dataset)\n",
    "        if __name__ == '__main__':\n",
    "                model.save(f\"model_{hidden_node}.h5\")\n",
    "        final_loss = history.history['loss'][-1]\n",
    "        weights = model.get_weights()\n",
    "        return final_loss, weights[0], weights[1], weights[2], weights[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sum = pd.DataFrame(columns=['mse', 'hl_weights', 'hl_bias', 'ol_weights', 'ol_bias'])\n",
    "for n in range(1, 21):\n",
    "    a,b,c,d,e = modelling(train_dataset=train_dataset, \n",
    "                            test_dataset=test_dataset, \n",
    "                            epoch=100, \n",
    "                            loss_function='mse',\n",
    "                            metrics = 'mae', \n",
    "                            optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "                            activation='sigmoid',\n",
    "                            hidden_node=n)\n",
    "    model_sum.loc[n] = [a,b,c,d,e]                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sum.to_excel('model_train_sumary.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('model_5.h5')\n",
    "\n",
    "# Assuming `data` is your complete time series data and has already been standardized\n",
    "N_PAST = 34\n",
    "N_FUTURE = 34\n",
    "num_future_periods = 136  # Number of future observations you want to predict\n",
    "\n",
    "# Starting with the last 34 observations from your standardized data\n",
    "current_input = data_norm[-N_PAST:].reshape((1, N_PAST, N_FEATURES))\n",
    "\n",
    "# Placeholder for all future predictions\n",
    "future_predictions = []\n",
    "\n",
    "for _ in range(num_future_periods // N_FUTURE):\n",
    "    # Make the forecast for the next N_FUTURE periods\n",
    "    prediction = model.predict(current_input)\n",
    "    \n",
    "    # Append the prediction to the list\n",
    "    future_predictions.append(prediction)\n",
    "    \n",
    "    # Prepare the input for the next prediction\n",
    "    current_input = np.append(current_input[:, N_FUTURE:, :], prediction, axis=1)\n",
    "\n",
    "# Convert list of arrays into a single array\n",
    "future_predictions = np.concatenate(future_predictions, axis=1)\n",
    "\n",
    "# If you need the predictions in the original scale, remember to inverse transform\n",
    "# future_predictions = scaler.inverse_transform(future_predictions)\n",
    "print(future_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
